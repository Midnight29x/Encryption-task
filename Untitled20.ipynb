{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw9PnzF8PdfdMi48XI4gxR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Midnight29x/Encryption-task/blob/main/Untitled20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42MYV7GI-NpX",
        "outputId": "75e787f1-2aca-4cf2-c7c7-452dd63fa9aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "KXTBKXawDrjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "EJsEb1ha-Kuq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Dataset Preparation:      \n",
        "1.1.Download and extract the Reuters News Dataset."
      ],
      "metadata": {
        "id": "AFc33nGED3OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Preparation\n",
        "documents = reuters.fileids()\n",
        "train_docs = [doc for doc in documents if doc.startswith(\"train\")]\n",
        "test_docs = [doc for doc in documents if doc.startswith(\"test\")]\n",
        "\n",
        "train_data = [reuters.raw(doc_id) for doc_id in train_docs]\n",
        "train_labels = [reuters.categories(doc_id)[0] for doc_id in train_docs]\n",
        "test_data = [reuters.raw(doc_id) for doc_id in test_docs]\n",
        "test_labels = [reuters.categories(doc_id)[0] for doc_id in test_docs]"
      ],
      "metadata": {
        "id": "Hx1sm0XCAqhY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2.Preprocess the text data by removing stop words, stemming, and lemmatizing, as per your preference."
      ],
      "metadata": {
        "id": "rnwiR1D1EAz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Text Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "train_data = [preprocess_text(text) for text in train_data]\n",
        "test_data = [preprocess_text(text) for text in test_data]"
      ],
      "metadata": {
        "id": "_o6Rw0WCAqp-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3.Split the datasets into training and testing sets in a ratio of 80:20.    \n",
        "**2.Feature Extraction:**     \n",
        "2.1.Convert the preprocessed text data into numerical vectors using one of the following feature extraction techniques: Bag of Words, TF-IDF, or Word Embeddings."
      ],
      "metadata": {
        "id": "31YRPFJdEI7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature Extraction\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vectors = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test_vectors = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "# Convert labels to categorical\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "y_train_encoded = onehot_encoder.fit_transform(y_train_encoded.reshape(-1, 1))\n",
        "y_test_encoded = onehot_encoder.transform(y_test_encoded.reshape(-1, 1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP5p-k5G9kkU",
        "outputId": "d4ad7127-b72d-484b-a863-04e3b82e7de2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Model Implementation:**    \n",
        "3.1.Implement Simple RNN."
      ],
      "metadata": {
        "id": "JzIvaaokEZ2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Implementation\n",
        "num_classes = len(set(train_labels))\n",
        "\n",
        "# Simple RNN\n",
        "simple_rnn_model = Sequential()\n",
        "simple_rnn_model.add(Embedding(X_train_vectors.shape[1], 100))\n",
        "simple_rnn_model.add(SimpleRNN(100))\n",
        "simple_rnn_model.add(Dense(num_classes, activation='softmax'))\n",
        "simple_rnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "simple_rnn_model.fit(X_train_vectors, y_train_encoded, epochs=5, batch_size=64)\n",
        "simple_rnn_loss, simple_rnn_acc = simple_rnn_model.evaluate(X_test_vectors, y_test_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jEFOhRTAqbB",
        "outputId": "690a4882-0831-45a6-e712-abe708083a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "22/98 [=====>........................] - ETA: 1:05:57 - loss: 3.2679 - accuracy: 0.3281"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2.Implement LSTM"
      ],
      "metadata": {
        "id": "HWo31pDwEl7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_train_padded = pad_sequences(X_train_sequences)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=X_train_padded.shape[1])\n",
        "\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(len(tokenizer.word_index) + 1, 100))\n",
        "lstm_model.add(LSTM(100))\n",
        "lstm_model.add(Dense(num_classes, activation='softmax'))\n",
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train_padded, y_train_encoded, epochs=10, batch_size=64)\n",
        "lstm_loss, lstm_acc = lstm_model.evaluate(X_test_padded, y_test_encoded)\n"
      ],
      "metadata": {
        "id": "poPUDQlRC3HQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Model Comparison:**      \n",
        "4.1.Compare the performance of both models based on accuracy and loss values"
      ],
      "metadata": {
        "id": "bR-RLcP0Ex9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Comparison\n",
        "print(\"Simple RNN - Accuracy:\", simple_rnn_acc, \"Loss:\", simple_rnn_loss)\n",
        "print(\"LSTM - Accuracy:\", lstm_acc, \"Loss:\", lstm_loss)\n"
      ],
      "metadata": {
        "id": "6d6Uu6SMEnmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4.2.Show accuracy and loss of both models in a graph using Matplotlib."
      ],
      "metadata": {
        "id": "Ko8ueYRBE3HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualize Results\n",
        "plt.plot(simple_rnn_model.history.history['accuracy'], label='Simple RNN')\n",
        "plt.plot(lstm_model.history.history['accuracy'], label='LSTM')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(simple_rnn_model.history.history['loss'], label='Simple RNN')\n",
        "plt.plot(lstm_model.history.history['loss'], label='LSTM')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xnOt0M3XEnib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}